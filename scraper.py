#!/usr/bin/env python3
"""
äºŒå­ç‰å·ãƒ©ã‚¤ã‚º ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³æ¯”è¼ƒè¡¨ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4
ä¾¡æ ¼å¤‰é·è¿½è·¡æ©Ÿèƒ½è¿½åŠ 
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional
import time
import sys
from price_tracker import PriceTracker
from html_generator import generate_html

# æ—¥æœ¬æ™‚é–“ï¼ˆJSTï¼‰ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))

# è‡ªå‹•æ¤œå‡ºã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
AUTO_DISCOVER = True  # False ã«ã™ã‚‹ã¨æ‰‹å‹•æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ã«ãªã‚‹

# æ‰‹å‹•æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ç”¨ï¼ˆAUTO_DISCOVER=False ã®å ´åˆã®ã¿ä½¿ç”¨ï¼‰
MANUAL_PROPERTY_URLS = [
    "https://www.livable.co.jp/mansion/C13252K32/",
    "https://www.livable.co.jp/mansion/C48258711/",
    "https://www.livable.co.jp/mansion/C1325X119/",
    "https://www.livable.co.jp/mansion/C13259K25/",
    "https://www.livable.co.jp/mansion/CV623ZG20/",
    "https://www.livable.co.jp/mansion/C48259B69/",
]

# é¢ç©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆè‡ªå‹•æ¤œå‡ºæ™‚ã®ã¿æœ‰åŠ¹ï¼‰
MIN_AREA = 0.0    # æœ€å°é¢ç©ï¼ˆã¡ï¼‰â€»åˆ¶é™ãªã—
MAX_AREA = 999.0  # æœ€å¤§é¢ç©ï¼ˆã¡ï¼‰â€»åˆ¶é™ãªã—

# ã‚·ãƒ¼ãƒ‰URLï¼ˆè‡ªå‹•æ¤œå‡ºã®èµ·ç‚¹ã¨ãªã‚‹ç‰©ä»¶ï¼‰
SEED_URLS = [
    "https://www.livable.co.jp/mansion/C13252K32/",  # ã‚¤ãƒ¼ã‚¹ãƒˆ
    "https://www.livable.co.jp/mansion/C48258711/",  # ã‚¦ã‚¨ã‚¹ãƒˆ
    "https://www.livable.co.jp/mansion/C13259K25/",  # ã‚»ãƒ³ãƒˆãƒ©ãƒ«
]


def extract_property_id(url: str) -> Optional[str]:
    """URLã‹ã‚‰ç‰©ä»¶IDã‚’æŠ½å‡º"""
    match = re.search(r'/mansion/(C[A-Z0-9]+)/?', url)
    if match:
        return match.group(1)
    return None


def discover_related_properties(property_url: str, visited: set, headers: dict) -> List[str]:
    """ç‰©ä»¶ãƒšãƒ¼ã‚¸ã‹ã‚‰ã€ŒåŒã˜å»ºç‰©ã®ä»–ã®ç‰©ä»¶ã€ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
    if not property_url.startswith('http'):
        property_url = f"https://www.livable.co.jp{property_url}"
    
    property_id = extract_property_id(property_url)
    if not property_id or property_url in visited:
        return []
    
    visited.add(property_url)
    
    try:
        response = requests.get(property_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        related_links = []
        
        # ã€Œä»–ã®ç‰©ä»¶ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        headings = soup.find_all(['h2', 'h3'], string=re.compile(r'ä»–ã®ç‰©ä»¶|ã“ã®å»ºç‰©|ã‚¤ãƒ¼ã‚¹ãƒˆ|ã‚¦ã‚¨ã‚¹ãƒˆ|ã‚»ãƒ³ãƒˆãƒ©ãƒ«'))
        
        for heading in headings:
            next_elem = heading.find_next(['ul', 'div', 'section'])
            if next_elem:
                links = next_elem.find_all('a', href=re.compile(r'/mansion/C[A-Z0-9]+'))
                for link in links:
                    href = link.get('href')
                    if href and '/mansion/C' in href:
                        if href.startswith('/'):
                            href = f"https://www.livable.co.jp{href}"
                        related_links.append(href)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ç‰©ä»¶ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        if not related_links:
            for link in soup.find_all('a', href=re.compile(r'/mansion/C[A-Z0-9]+')):
                href = link.get('href')
                if href:
                    if any(x in href for x in ['mailto:', 'javascript:', 'line.me', 'twitter.com', 'facebook.com']):
                        continue
                    if href.startswith('/'):
                        href = f"https://www.livable.co.jp{href}"
                    if href != property_url:
                        related_links.append(href)
        
        return list(set(related_links))
        
    except Exception as e:
        return []


def auto_discover_properties(seed_urls: List[str], headers: dict) -> List[str]:
    """ã‚·ãƒ¼ãƒ‰URLã‹ã‚‰å¹…å„ªå…ˆæ¢ç´¢ã§å…¨ç‰©ä»¶ã‚’ç™ºè¦‹"""
    from collections import deque
    
    queue = deque(seed_urls)
    visited = set()
    discovered = set()
    
    print("="*60)
    print("ç‰©ä»¶è‡ªå‹•æ¤œå‡ºã‚’é–‹å§‹...")
    print("="*60)
    
    while queue:
        current_url = queue.popleft()
        
        if not current_url.startswith('http'):
            current_url = f"https://www.livable.co.jp{current_url}"
        
        property_id = extract_property_id(current_url)
        if not property_id or current_url in discovered:
            continue
        
        discovered.add(current_url)
        print(f"  âœ“ {property_id} ã‚’ç™ºè¦‹ ({len(discovered)}ä»¶ç›®)")
        
        related = discover_related_properties(current_url, visited, headers)
        
        for url in related:
            if url not in discovered:
                queue.append(url)
        
        if queue:
            time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆè‡ªå‹•æ¤œå‡ºæ™‚ã¯å°‘ã—é€Ÿãï¼‰
    
    print(f"  â†’ åˆè¨ˆ {len(discovered)} ä»¶ã®ç‰©ä»¶ã‚’ç™ºè¦‹")
    print("="*60)
    print()
    
    return sorted(list(discovered))


def find_dl_value(soup: BeautifulSoup, keyword: str) -> Optional[str]:
    """dl/dt/ddæ§‹é€ ã‹ã‚‰ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹å€¤ã‚’å–å¾—"""
    dt_elem = soup.find('dt', string=re.compile(keyword, re.IGNORECASE))
    if dt_elem:
        dd_elem = dt_elem.find_next_sibling('dd')
        if dd_elem:
            return dd_elem.get_text(strip=True)
    return None


def scrape_property(url: str) -> Dict:
    """ç‰©ä»¶æƒ…å ±ã‚’å–å¾—"""
    print(f"å–å¾—ä¸­: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç®¡ç†ç•ªå·ï¼ˆURLã‹ã‚‰å–å¾—ï¼‰
        kanri_no = url.rstrip('/').split('/')[-1]
        
        property_data = {
            'url': url,
            'kanri_no': kanri_no,
        }
        
        # ä¾¡æ ¼ã®æŠ½å‡º
        price_value = find_dl_value(soup, r'ä¾¡æ ¼')
        if price_value:
            # "1å„„5,800ä¸‡å††" ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œ
            if 'å„„' in price_value:
                oku_match = re.search(r'(\d+)å„„([\d,]+)?ä¸‡å††', price_value)
                if oku_match:
                    oku = int(oku_match.group(1)) * 10000
                    man = int(oku_match.group(2).replace(',', '')) if oku_match.group(2) else 0
                    property_data['price'] = oku + man
            else:
                price_match = re.search(r'([\d,]+)ä¸‡å††', price_value)
                if price_match:
                    property_data['price'] = int(price_match.group(1).replace(',', ''))
        
        # é–“å–ã‚Šã®æŠ½å‡º
        madori_value = find_dl_value(soup, r'é–“å–ã‚Š')
        if madori_value:
            property_data['madori'] = madori_value.strip()
        
        # å°‚æœ‰é¢ç©ã®æŠ½å‡º
        area_value = find_dl_value(soup, r'å°‚æœ‰é¢ç©')
        if area_value:
            # "å£èŠ¯70.32m2" ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
            area_match = re.search(r'([\d.]+)\s*m', area_value)
            if area_match:
                property_data['area'] = float(area_match.group(1))
        
        # æ‰€åœ¨åœ°ã‹ã‚‰æ£Ÿåã‚’æŠ½å‡º
        location_value = find_dl_value(soup, r'æ‰€åœ¨åœ°')
        if location_value:
            if 'ã‚¤ãƒ¼ã‚¹ãƒˆ' in location_value:
                property_data['building'] = 'ã‚¤ãƒ¼ã‚¹ãƒˆ'
            elif 'ã‚¦ã‚¨ã‚¹ãƒˆ' in location_value:
                property_data['building'] = 'ã‚¦ã‚¨ã‚¹ãƒˆ'
            elif 'ã‚»ãƒ³ãƒˆãƒ©ãƒ«' in location_value:
                property_data['building'] = 'ã‚»ãƒ³ãƒˆãƒ©ãƒ«'
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚‚æ£Ÿåã‚’æŠ½å‡ºï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
        if 'building' not in property_data:
            title = soup.find('h1')
            if title:
                title_text = title.get_text()
                if 'ã‚¤ãƒ¼ã‚¹ãƒˆ' in title_text:
                    property_data['building'] = 'ã‚¤ãƒ¼ã‚¹ãƒˆ'
                elif 'ã‚¦ã‚¨ã‚¹ãƒˆ' in title_text:
                    property_data['building'] = 'ã‚¦ã‚¨ã‚¹ãƒˆ'
                elif 'ã‚»ãƒ³ãƒˆãƒ©ãƒ«' in title_text:
                    property_data['building'] = 'ã‚»ãƒ³ãƒˆãƒ©ãƒ«'
        
        # éšæ•°ã®æŠ½å‡º
        floor_value = find_dl_value(soup, r'æ‰€åœ¨éšæ•°|æ‰€åœ¨éš')
        if floor_value:
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆå¾Œã§å‰Šé™¤ï¼‰
            # print(f"  DEBUG: floor_value = {repr(floor_value)}")
            # "17éš / åœ°ä¸Š42éš åœ°ä¸‹1éš" ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ "17/42" ã‚’æŠ½å‡º
            floor_match = re.search(r'(\d+)éš.*?åœ°ä¸Š(\d+)éš', floor_value)
            if floor_match:
                property_data['floor'] = f"{floor_match.group(1)}/{floor_match.group(2)}"
            else:
                # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³: "17éš" ã®ã¿ã§ã‚‚å–å¾—
                simple_floor_match = re.search(r'(\d+)éš', floor_value)
                if simple_floor_match:
                    property_data['floor'] = simple_floor_match.group(1)
        
        # ç¯‰å¹´æœˆã®æŠ½å‡º
        built_value = find_dl_value(soup, r'ç¯‰å¹´æœˆ')
        if built_value:
            property_data['built'] = built_value.strip()
        
        # å‘ãã®æŠ½å‡º
        direction_value = find_dl_value(soup, r'å‘ã')
        if direction_value:
            property_data['direction'] = direction_value.strip()
        
        # ãƒªãƒ•ã‚©ãƒ¼ãƒ ã®æŠ½å‡ºï¼ˆè¨­å‚™ãƒ»æ¡ä»¶ã‚„å‚™è€ƒã‹ã‚‰æ¨æ¸¬ï¼‰
        reform_value = find_dl_value(soup, r'ãƒªãƒ•ã‚©ãƒ¼ãƒ |ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³')
        equipment_value = find_dl_value(soup, r'è¨­å‚™ãƒ»æ¡ä»¶')
        remarks_value = find_dl_value(soup, r'å‚™è€ƒ')
        
        property_data['reform'] = 'ç„¡'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€Œç„¡ã€
        
        # ãƒªãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
        for value in [reform_value, equipment_value, remarks_value]:
            if value and ('ãƒªãƒ•ã‚©ãƒ¼ãƒ ' in value or 'ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³' in value or 'æ”¹ä¿®' in value):
                if 'æ¸ˆ' in value or 'æœ‰' in value:
                    property_data['reform'] = 'æœ‰'
                    break
        
        # ãŠæ°—ã«å…¥ã‚Šç™»éŒ²æ•°ã®æŠ½å‡º
        favorite_count = None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€Œâ—¯ä»¶ ãŠæ°—ã«å…¥ã‚Šã€ã®ã‚ˆã†ãªè¡¨è¨˜ã‚’æ¢ã™
        favorite_text = soup.find(string=re.compile(r'ä»¶\s*ãŠæ°—ã«å…¥ã‚Š'))
        if favorite_text:
            # ã€Œ30ä»¶ ãŠæ°—ã«å…¥ã‚Šã€ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
            parent_text = favorite_text.parent.get_text(strip=True) if favorite_text.parent else str(favorite_text)
            count_match = re.search(r'(\d+)\s*ä»¶\s*ãŠæ°—ã«å…¥ã‚Š', parent_text)
            if count_match:
                favorite_count = int(count_match.group(1))
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒ¼ãƒˆã‚¢ã‚¤ã‚³ãƒ³ã®è¿‘ãã«ã‚ã‚‹æ•°å€¤ã‚’æ¢ã™
        if favorite_count is None:
            # classåã«ã€Œfavoriteã€ã‚„ã€Œlikeã€ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            favorite_elems = soup.find_all(['span', 'div', 'p'], class_=re.compile(r'favorite|like', re.IGNORECASE))
            for elem in favorite_elems:
                text = elem.get_text(strip=True)
                # ã€Œ1 ä»¶ ãŠæ°—ã«å…¥ã‚Šã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                count_match = re.search(r'(\d+)\s*ä»¶', text)
                if count_match and 'ãŠæ°—ã«å…¥ã‚Š' in text:
                    favorite_count = int(count_match.group(1))
                    break
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚ˆã‚Šåºƒç¯„å›²ã«ã€ŒãŠæ°—ã«å…¥ã‚Šã€ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆã®è¿‘ãã‚’æ¢ã™
        if favorite_count is None:
            all_text = soup.get_text()
            # ã€Œ30 å›é–²è¦§ã€ã¨ã€Œ1 ä»¶ ãŠæ°—ã«å…¥ã‚Šã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            matches = re.finditer(r'(\d+)\s*ä»¶\s*ãŠæ°—ã«å…¥ã‚Š', all_text)
            for match in matches:
                favorite_count = int(match.group(1))
                break
        
        property_data['favorite_count'] = favorite_count if favorite_count is not None else 0
        
        # æ‹…å½“è€…ã®æŠ½å‡º
        contact_section = soup.find('p', string=re.compile(r'ç§ãŒæ‹…å½“'))
        if contact_section:
            staff_elem = contact_section.find_next('p')
            if staff_elem:
                staff_name = staff_elem.get_text(strip=True)
                if staff_name:
                    property_data['staff'] = staff_name
        
        # æ‹…å½“è€…ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åˆ¥ã®æ–¹æ³•ã§æ¢ã™
        if 'staff' not in property_data:
            staff_elems = soup.find_all('p', class_=re.compile('contact-person__name'))
            for elem in staff_elems:
                text = elem.get_text(strip=True)
                if text and not re.match(r'^[a-zA-Z]+$', text):  # ãƒ­ãƒ¼ãƒå­—èª­ã¿ã‚’é™¤å¤–
                    property_data['staff'] = text
                    break
        
        # å¹³ç±³å˜ä¾¡ãƒ»åªå˜ä¾¡ã®è¨ˆç®—
        if 'price' in property_data and 'area' in property_data:
            property_data['price_per_sqm'] = property_data['price'] / property_data['area']
            property_data['price_per_tsubo'] = property_data['price'] / (property_data['area'] / 3.3)
        
        # print(f"âœ“ {kanri_no}: å–å¾—æˆåŠŸ")  # mainé–¢æ•°å´ã§å‡ºåŠ›
        return property_data
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return {'url': url, 'kanri_no': url.rstrip('/').split('/')[-1], 'error': str(e)}


def generate_comparison_table(properties: List[Dict]) -> str:
    """æ¯”è¼ƒè¡¨ã‚’ç”Ÿæˆ"""
    
    # ã‚¨ãƒ©ãƒ¼ãŒãªã„ç‰©ä»¶ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    valid_properties = [p for p in properties if 'error' not in p and 'price_per_sqm' in p]
    
    if not valid_properties:
        return "# ã‚¨ãƒ©ãƒ¼\n\næœ‰åŠ¹ãªç‰©ä»¶æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    # å¹³ç±³å˜ä¾¡ã§ã‚½ãƒ¼ãƒˆ
    sorted_properties = sorted(valid_properties, key=lambda x: x.get('price_per_sqm', float('inf')))
    
    # Markdownè¡¨ã®ä½œæˆ
    output = []
    output.append("# äºŒå­ç‰å·ãƒ©ã‚¤ã‚º ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³æ¯”è¼ƒè¡¨\n")
    output.append(f"**ä½œæˆæ—¥æ™‚**: {datetime.now(JST).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n")
    output.append("## æ¯”è¼ƒè¡¨ï¼ˆå¹³ç±³å˜ä¾¡é †ï¼‰\n")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
    headers = ["ç®¡ç†ç•ªå·", "è²©å£²ä¾¡æ ¼", "å¹³ç±³å˜ä¾¡", "åªå˜ä¾¡", "é–“å–ã‚Š", "å°‚æœ‰é¢ç©", "æ£Ÿå", "éšæ•°", "ç¯‰å¹´æœˆ", "å‘ã", "ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "ãŠæ°—ã«å…¥ã‚Š", "æ‹…å½“è€…"]
    output.append("| " + " | ".join(headers) + " |")
    output.append("|" + "|".join(["---"] * len(headers)) + "|")
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    for prop in sorted_properties:
        row = [
            prop.get('kanri_no', '-'),
            f"{prop.get('price', 0):,}ä¸‡å††" if 'price' in prop else '-',
            f"{prop.get('price_per_sqm', 0):.2f}ä¸‡å††/ã¡" if 'price_per_sqm' in prop else '-',
            f"{prop.get('price_per_tsubo', 0):.1f}ä¸‡å††/åª" if 'price_per_tsubo' in prop else '-',
            prop.get('madori', '-'),
            f"{prop.get('area', 0):.2f}ã¡" if 'area' in prop else '-',
            prop.get('building', '-'),
            prop.get('floor', '-'),
            prop.get('built', '-'),
            prop.get('direction', '-'),
            prop.get('reform', '-'),
            str(prop.get('favorite_count', '-')),
            prop.get('staff', '-'),
        ]
        output.append("| " + " | ".join(row) + " |")
    
    # ç‰©ä»¶ãƒªãƒ³ã‚¯ä¸€è¦§
    output.append("\n## äºŒå­ç‰å·ãƒ©ã‚¤ã‚º ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³ç‰©ä»¶ä¸€è¦§\n")
    
    for i, prop in enumerate(sorted_properties, 1):
        area_str = f"{prop.get('area', 0):.2f}ã¡" if 'area' in prop else "-"
        output.append(f"{i}. **{prop.get('kanri_no', '-')}** - {prop.get('building', '-')} {prop.get('floor', '-')} {prop.get('madori', '-')} {area_str}")
        output.append(f"   {prop.get('url', '')}\n")
    
    # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸç‰©ä»¶ã‚’è¡¨ç¤º
    error_properties = [p for p in properties if 'error' in p]
    if error_properties:
        output.append("\n## å–å¾—ã«å¤±æ•—ã—ãŸç‰©ä»¶\n")
        for prop in error_properties:
            output.append(f"- **{prop.get('kanri_no', '-')}**: {prop.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            output.append(f"  {prop.get('url', '')}\n")
    
    # å•ã„åˆã‚ã›å…ˆ
    output.append("\n---\n")
    output.append("**å•ã„åˆã‚ã›å…ˆ:**  ")
    output.append("æ±æ€¥ãƒªãƒãƒ–ãƒ« äºŒå­ç‰å·ã‚»ãƒ³ã‚¿ãƒ¼  ")
    output.append("TEL: 0120-938-291ï¼ˆç„¡æ–™ï¼‰  ")
    output.append("å–¶æ¥­æ™‚é–“: 10:00ï½18:00  ")
    output.append("å®šä¼‘æ—¥: æ¯é€±ç«æ›œæ—¥ãƒ»æ°´æ›œæ—¥")
    
    return "\n".join(output)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("äºŒå­ç‰å·ãƒ©ã‚¤ã‚º ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³æ¯”è¼ƒè¡¨ä½œæˆ v4")
    print("=" * 60)
    print()
    
    # ä¾¡æ ¼è¿½è·¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
    tracker = PriceTracker()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # ç‰©ä»¶URLã‚’å–å¾—ï¼ˆè‡ªå‹•æ¤œå‡º or æ‰‹å‹•æŒ‡å®šï¼‰
    all_urls = []  # ç™ºè¦‹ã—ãŸå…¨ç‰©ä»¶ã®URL
    
    if AUTO_DISCOVER:
        print("ğŸ” è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰")
        all_urls = auto_discover_properties(SEED_URLS, headers)
        
        # æ£Ÿåã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã¾ãšå…¨ç‰©ä»¶ã®æƒ…å ±ã‚’å–å¾—ï¼‰
        print("ğŸ“ æ£Ÿåãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
        print(f"   æ¡ä»¶: äºŒå­ç‰å·ãƒ©ã‚¤ã‚ºã®ã¿ï¼ˆã‚¤ãƒ¼ã‚¹ãƒˆ/ã‚¦ã‚¨ã‚¹ãƒˆ/ã‚»ãƒ³ãƒˆãƒ©ãƒ«ï¼‰")
        print()
        
        filtered_properties = []
        for i, url in enumerate(all_urls, 1):
            property_id = extract_property_id(url)
            print(f"[{i}/{len(all_urls)}] {property_id}: ", end="", flush=True)
            
            property_data = scrape_property(url)
            
            # é¢ç©ã¨æ£Ÿåã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if 'area' in property_data:
                area = property_data['area']
                building = property_data.get('building', '')
                
                # é¢ç©ãƒã‚§ãƒƒã‚¯
                if MIN_AREA <= area < MAX_AREA:
                    # æ£Ÿåãƒã‚§ãƒƒã‚¯ï¼ˆäºŒå­ç‰å·ãƒ©ã‚¤ã‚ºã®ã¿ï¼‰
                    if building in ['ã‚¤ãƒ¼ã‚¹ãƒˆ', 'ã‚¦ã‚¨ã‚¹ãƒˆ', 'ã‚»ãƒ³ãƒˆãƒ©ãƒ«']:
                        print(f"âœ“ {area:.2f}ã¡ {building} - æ¡ä»¶ã«åˆè‡´")
                        filtered_properties.append(property_data)
                    else:
                        print(f"âœ— {area:.2f}ã¡ æ£Ÿåãªã— - äºŒå­ç‰å·ãƒ©ã‚¤ã‚ºä»¥å¤–")
                else:
                    print(f"âœ— {area:.2f}ã¡ - ç¯„å›²å¤–")
            else:
                print("âœ— é¢ç©æƒ…å ±ãªã—")
            
            if i < len(all_urls):
                time.sleep(1)
        
        properties = filtered_properties
        
        print()
        print("="*60)
        print(f"ãƒ•ã‚£ãƒ«ã‚¿çµæœ: {len(properties)}/{len(all_urls)} ä»¶ãŒæ¡ä»¶ã«åˆè‡´")
        print("="*60)
        print()
        
    else:
        print("ğŸ“ æ‰‹å‹•æŒ‡å®šãƒ¢ãƒ¼ãƒ‰")
        print(f"   å¯¾è±¡: {len(MANUAL_PROPERTY_URLS)} ä»¶")
        print()
        
        all_urls = MANUAL_PROPERTY_URLS
        
        # å„ç‰©ä»¶æƒ…å ±ã‚’å–å¾—
        properties = []
        for i, url in enumerate(MANUAL_PROPERTY_URLS, 1):
            property_id = extract_property_id(url)
            print(f"[{i}/{len(MANUAL_PROPERTY_URLS)}] {property_id}: ", end="", flush=True)
            property_data = scrape_property(url)
            properties.append(property_data)
            print()
            
            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å°‘ã—å¾…æ©Ÿ
            if i < len(MANUAL_PROPERTY_URLS):
                time.sleep(1)
    
    # å¤‰æ›´æ¤œå‡º
    print("=" * 60)
    print("å¤‰æ›´æ¤œå‡ºä¸­...")
    changes = tracker.detect_changes(properties)
    
    has_changes = any(changes.values())
    if has_changes:
        print("âœ“ å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        print(f"  - ä¾¡æ ¼å¤‰æ›´: {len(changes['price_changes'])}ä»¶")
        print(f"  - æ–°è¦ç‰©ä»¶: {len(changes['new_properties'])}ä»¶")
        print(f"  - è²©å£²çµ‚äº†: {len(changes['ended_properties'])}ä»¶")
        print(f"  - æ‹…å½“è€…å¤‰æ›´: {len(changes['staff_changes'])}ä»¶")
    else:
        print("å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print("=" * 60)
    print()
    
    # å¤‰æ›´ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    if has_changes:
        change_report = tracker.generate_change_report(changes)
        change_report_file = f"changes_{datetime.now(JST).strftime('%Y%m%d')}.md"
        with open(change_report_file, 'w', encoding='utf-8') as f:
            f.write(change_report)
        print(f"âœ“ å¤‰æ›´ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ: {change_report_file}\n")
        print(change_report)
        print()
    
    # ä¾¡æ ¼å±¥æ­´ã‚’ä¿å­˜
    tracker.save_current_data(properties)
    print("âœ“ ä¾¡æ ¼å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: price_tracker.json\n")
    
    # æ¯”è¼ƒè¡¨ã‚’ç”Ÿæˆ
    print("=" * 60)
    print("æ¯”è¼ƒè¡¨ã‚’ç”Ÿæˆä¸­...")
    comparison_table = generate_comparison_table(properties)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    output_file = f"comparison_table_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(comparison_table)
    
    # latest.md ã‚‚æ›´æ–°
    with open('latest.md', 'w', encoding='utf-8') as f:
        f.write(comparison_table)
    
    # index.html ã‚’ç”Ÿæˆ
    html_content = generate_html(properties, total_discovered=len(all_urls) if AUTO_DISCOVER else len(MANUAL_PROPERTY_URLS))
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ“ æ¯”è¼ƒè¡¨ã‚’ä½œæˆã—ã¾ã—ãŸ: {output_file}")
    print(f"âœ“ latest.md ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    print(f"âœ“ index.html ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    print("=" * 60)
    print()
    print(comparison_table)


if __name__ == "__main__":
    main()

